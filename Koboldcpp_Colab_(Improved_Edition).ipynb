{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#CELL 1\n",
        "#@title Keep this widget playing to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "kaXZpNRbLwlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBYypQ1CIk0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import threading\n",
        "import time\n",
        "\n",
        "#@title Koboldcpp Colab (Improved Edition)\n",
        "\n",
        "# URL of the built koboldcpp folder\n",
        "url = \"https://huggingface.co/kalomaze/ColabDependencies/resolve/main/content/koboldcpp.tar.gz\"\n",
        "\n",
        "Model = \"MythoMax-L2-13B-GGUF\" #@param [\"MythoMax-L2-13B-GGUF\", \"ReMM-SLERP-L2-13B-GGUF\", \"Stheno-L2-13B-GGUF\"]\n",
        "Quant_Method = \"5_K_M\" #@param [\"4_K_M\", \"5_K_M\"]\n",
        "Layers = 43 #@param [43]{allow-input: true}\n",
        "Context = 4096 #@param [4096]{allow-input: true}\n",
        "\n",
        "#@markdown ### OPTIONAL: Manual Model Link\n",
        "Use_Manual_Model = True #@param {type:\"boolean\"}\n",
        "Manual_Link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### OPTIONAL: Build Latest Kobold (takes ~7 minutes)\n",
        "Force_Update_Build = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_links = {\n",
        "    \"MythoMax-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q{}.gguf\",\n",
        "    \"ReMM-SLERP-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/ReMM-SLERP-L2-13B-GGUF/resolve/main/remm-slerp-l2-13b.Q{}.gguf\",\n",
        "    \"Stheno-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/Stheno-L2-13B-GGUF/resolve/main/stheno-l2-13b.Q{}.gguf\"\n",
        "}\n",
        "\n",
        "if Use_Manual_Model:\n",
        "    if Manual_Link.strip() != \"\":\n",
        "        print(f\"\\nManual Model detected; will use {Manual_Link} instead of {Model}\\n\")\n",
        "        Model = Manual_Link\n",
        "    else:\n",
        "        print(f\"\\nWarning: Manual Model enabled, but no link was found. Falling back to {Model}\\n\")\n",
        "        if Model in model_links:\n",
        "            Model = model_links[Model].format(Quant_Method)\n",
        "else:\n",
        "    if Model in model_links:\n",
        "        Model = model_links[Model].format(Quant_Method)\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists('/content/koboldcpp/'):\n",
        "    os.makedirs('/content/koboldcpp/')\n",
        "\n",
        "# Checking if you already downloaded Kobold\n",
        "if not os.path.exists(\"/content/koboldcpp.tar.gz\"):\n",
        "    if Force_Update_Build == False:\n",
        "        response = requests.get(url, stream=True)\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        with open(filename, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "\n",
        "        with tarfile.open(filename, 'r:gz') as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if member.name.startswith('koboldcpp'):\n",
        "                    try:\n",
        "                        tar.extract(member, path='/content')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error extracting '{member.name}': {str(e)}\")\n",
        "\n",
        "        print(\"File extraction to /content/ completed!\")\n",
        "    else:\n",
        "        print(\"Skipping prebuilt kobold, will build manually...\")\n",
        "        !git clone https://github.com/LostRuins/koboldcpp\n",
        "        %cd /content/koboldcpp\n",
        "        !make LLAMA_CUBLAS=1\n",
        "\n",
        "# Change to the directory\n",
        "%cd /content/koboldcpp\n",
        "\n",
        "# Hosting the cloudflared server\n",
        "!wget -c https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!echo > nohup.out\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:5001 &\n",
        "!sleep 10\n",
        "!cat nohup.out\n",
        "\n",
        "# Download the file if it doesn't exist already\n",
        "if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !wget $Model -O model.gguf\n",
        "\n",
        "if os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !rm koboldcpp.py\n",
        "    !wget https://cdn.discordapp.com/attachments/1132396046480314388/1150324006407376927/koboldcpp.py\n",
        "    !python koboldcpp.py model.gguf --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo\n",
        "else:\n",
        "    print(\"Failed to download the GGUF model. Please retry.\")"
      ]
    }
  ]
}